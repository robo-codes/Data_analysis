{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Churn Prediction Analysis Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt#visualization\n",
    "from PIL import  Image\n",
    "%matplotlib inline\n",
    "import seaborn as sns#visualization\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import plotly.graph_objs as go#visualization\n",
    "import plotly.offline as py#visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned file\n",
    "telcom = pd.read_csv(\"clean_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telcom.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "telcom = telcom.drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telcom.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#customer id col\n",
    "Id_col     = ['customerID']\n",
    "#Target columns\n",
    "target_col = [\"Churn\"]\n",
    "#categorical columns\n",
    "cat_cols   = telcom.nunique()[telcom.nunique() < 6].keys().tolist()\n",
    "cat_cols   = [x for x in cat_cols if x not in target_col]\n",
    "#numerical columns\n",
    "num_cols   = [x for x in telcom.columns if x not in cat_cols + target_col + Id_col]\n",
    "\n",
    "#Binary columns with 2 values\n",
    "bin_cols   = telcom.nunique()[telcom.nunique() == 2].keys().tolist()\n",
    "#Columns more than 2 values\n",
    "multi_cols = [i for i in cat_cols if i not in bin_cols]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label encoding Binary columns\n",
    "le = LabelEncoder()\n",
    "for i in bin_cols :\n",
    "    telcom[i] = le.fit_transform(telcom[i])\n",
    "    \n",
    "#Duplicating columns for multi value columns\n",
    "telcom = pd.get_dummies(data = telcom,columns = multi_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telcom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling Numerical columns\n",
    "std = StandardScaler()\n",
    "scaled = std.fit_transform(telcom[num_cols])\n",
    "scaled = pd.DataFrame(scaled,columns=num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telcom = telcom.drop(columns = num_cols,axis = 1)\n",
    "telcom = telcom.merge(scaled,left_index=True,right_index=True,how = \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the purpose of the above code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = telcom.describe().transpose()\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make it look nice we can do some additional stuff if needed\n",
    "summary = (telcom[[i for i in telcom.columns if i not in Id_col]].\n",
    "           describe().transpose().reset_index())\n",
    "\n",
    "summary = summary.rename(columns = {\"index\" : \"feature\"})\n",
    "summary = np.around(summary,3)\n",
    "\n",
    "val_lst = [summary['feature'], summary['count'],\n",
    "           summary['mean'],summary['std'],\n",
    "           summary['min'], summary['25%'],\n",
    "           summary['50%'], summary['75%'], summary['max']]\n",
    "\n",
    "trace  = go.Table(header = dict(values = summary.columns.tolist(),\n",
    "                                line = dict(color = ['#506784']),\n",
    "                                fill = dict(color = ['#119DFF']),\n",
    "                               ),\n",
    "                  cells  = dict(values = val_lst,\n",
    "                                line = dict(color = ['#506784']),\n",
    "                                fill = dict(color = [\"lightgrey\",'#F5F8FF'])\n",
    "                               ),\n",
    "                  columnwidth = [200,60,100,100,60,60,80,80,80])\n",
    "layout = go.Layout(dict(title = \"Variable Summary\"))\n",
    "figure = go.Figure(data=[trace],layout=layout)\n",
    "py.iplot(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation\n",
    "correlation = telcom.corr()\n",
    "#tick labels\n",
    "matrix_cols = correlation.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to array\n",
    "corr_array  = np.array(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting\n",
    "trace = go.Heatmap(z = corr_array,\n",
    "                   x = matrix_cols,\n",
    "                   y = matrix_cols,\n",
    "                   colorscale = \"Viridis\",\n",
    "                   colorbar   = dict(title = \"Pearson Correlation coefficient\",\n",
    "                                     titleside = \"right\"\n",
    "                                    ) ,\n",
    "                  )\n",
    "\n",
    "layout = go.Layout(dict(title = \"Correlation Matrix for variables\",\n",
    "                        autosize = False,\n",
    "                        height  = 720,\n",
    "                        width   = 800,\n",
    "                        margin  = dict(r = 0 ,l = 210,\n",
    "                                       t = 25,b = 210,\n",
    "                                      ),\n",
    "                        yaxis   = dict(tickfont = dict(size = 9)),\n",
    "                        xaxis   = dict(tickfont = dict(size = 9))\n",
    "                       )\n",
    "                  )\n",
    "\n",
    "data = [trace]\n",
    "fig = go.Figure(data=data,layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building (We will build Decision Tree and Logistics Regression models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features from the class label and split the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the modules\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import precision_score,recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting train and test data \n",
    "train,test = train_test_split(telcom,test_size = .25 ,random_state = 111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q. What is the purpose of random_state parameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##seperating dependent and independent variables\n",
    "cols    = [i for i in telcom.columns if i not in Id_col + target_col]\n",
    "X_train = train[cols]\n",
    "Y_train = train[target_col]\n",
    "X_test  = test[cols]\n",
    "Y_test  = test[target_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistics Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting test set\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the Results\n",
    "cm = confusion_matrix(Y_test, y_pred)\n",
    "accuracy_score(Y_test, y_pred)\n",
    "precision_score(Y_test, y_pred)\n",
    "recall_score(Y_test, y_pred)\n",
    "f1_score(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Q. What do the scores mean? Is this a good model fit based on the scores. Make sure you print all the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the confusion matrix\n",
    "df_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\n",
    "plt.figure(figsize = (10, 7))\n",
    "sns.heatmap(df_cm, annot = True, fmt ='g')\n",
    "print(\"Test Data Accuracy: %.4f\" %accuracy_score(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tree = DecisionTreeClassifier(random_state = 2)\n",
    "model_tree.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting test set\n",
    "y_pred = model_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the Results\n",
    "cm = confusion_matrix(Y_test, y_pred)\n",
    "accuracy_score(Y_test, y_pred)\n",
    "precision_score(Y_test, y_pred)\n",
    "recall_score(Y_test, y_pred)\n",
    "f1_score(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Q. What do the scores mean? Is this a good model fit based on the scores. Make sure you print all the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Confusion Matrix\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\n",
    "plt.figure(figsize = (10, 7))\n",
    "sns.heatmap(df_cm, annot = True, fmt ='g')\n",
    "print(\"Test Data Accuracy: %.4f\" %accuracy_score(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Q Which model performs better? (Hint: compare the metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K- fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q. What is K-fold cross validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X= X_train, y =Y_train, cv =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for accuracies\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q. What do accuracies tell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing Coefficients\n",
    "pd.concat([pd.DataFrame(X_train.columns, columns =[\"features\"]), pd.DataFrame(np.transpose(classifier.coef_), \n",
    "                                                                              columns =[\"Coef\"])], \n",
    "                                                                              axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Seclection/Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_selection import RFE \n",
    "classifier = LogisticRegression()\n",
    "rfe = RFE(classifier, 10)\n",
    "rfe = rfe.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rfe.support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns[rfe.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model again after Feature Selection\n",
    "classifier = LogisticRegression(random_state = 2)\n",
    "classifier.fit(X_train[X_train.columns[rfe.support_]], Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the Results\n",
    "cm = confusion_matrix(Y_test, y_pred)\n",
    "accuracy_score(Y_test, y_pred)\n",
    "precision_score(Y_test, y_pred)\n",
    "recall_score(Y_test, y_pred)\n",
    "f1_score(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\n",
    "plt.figure(figsize = (10, 7))\n",
    "sns.heatmap(df_cm, annot = True, fmt ='g')\n",
    "print(\"Test Data Accuracy: %.4f\" %accuracy_score(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q. Has the model improved after feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the coefficents for RFE\n",
    "pd.concat([pd.DataFrame(X_train.columns[rfe.support_], columns =[\"features\"]), pd.DataFrame(np.transpose(classifier.coef_), \n",
    "                                                                              columns =[\"Coef\"])], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = pd.concat([Y_test, telcom.customerID], axis =1).dropna()\n",
    "final_results['predicted_churn'] = y_pred\n",
    "final_results = final_results[['customerID', 'Churn', 'predicted_churn']].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q. Print the final Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q. Provide recommendations based on the feature selection. What should company target for to reduce churn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Q. Can you Add SVM model to this dataset and perform the same steps and check test data accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
